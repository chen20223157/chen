# 》》》windows音视频采集

## 一、DirectSound、WaveOut 和 WASAPI 的区别,三个音频程序的api

### 1、waveout:

是最早的音频播放api之一，属于WMApi的一部分，主要用于播放波形音频数据，例如WAV文件。

接口简单，兼容性高，没有缓冲和同步机制，在实时播放方面较弱，资源占用率低，适合基本音频播放

适用于需要快速实现基本的音频播放功能的应用程序，对音频质量要求不高的场景。

### 2、directSound:

是DirectX 中的，专门用于提高质量的音频处理能力，支持硬件加速，提供了音频混合和控制功能。

可以利用声卡的硬件加速，提高音频处理的效率质量，支持多声道输出，适用于复杂的场景，同步方面有更好的性能，更灵活的音频缓冲区管理，可以优化音频数据的传输和处理

适用于游戏和多媒体的高质量音频播放，复杂的音频混合的控制场景。

### 3、wasapi：

更高版本的新音频api，低延迟高质量，允许应用程序直接和音频设备通信，绕过中间层，减少音频数据的延迟和失帧。

保障实时性，低延迟，因为直接访问音频设备，能更快处理，保证保真度，支持音频会话管理，提供共享模式和独占模式。

android的音视频采集

---

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------

# 》》》OpenSLES和AudioRecord

### 1、OpenSLES

OpenSLES是一个跨平台的音频api，在安卓上通过NDK提供，用于c/c++开发，支持低延迟音频处理

支持跨平台性，可在Android，ios等系统使用，适合游戏音频，音乐应用的实时处理场景，可以绕过Java层直接处理PCM数据，支持播放，录制音效处理

### 2、AudioRecord

AudioRecord是androidSDK提供的Java层音频录制API，使用更简单，集成方便，适用于大部分的android应用开发，需要通过java缓冲区，语言java/kotlin

javaAPI，支持多种音频处理格式，自动处理录音权限，与activity/fragment的生命周期绑定。

# 》》》activity/fragment的生命周期绑定。

### 1、生命周期

生命周期管理主要就是音频资源和activity/fragment的生命周期事件（创建、暂停、恢复、销毁）自动或手动同步的管理机制。

### 2、Activity

**Activity是Android应用的单个屏幕界面**，相当于Web应用中的一个页面或桌面应用的一个窗口。

### 3、Fragment

**Fragment是Activity中的可重用UI模块**，一个Activity可以包含多个Fragment，实现灵活的UI设计。

---

-----

---

# 》》》audioQueue

apple平台的核心音频api，低级音频，c语言api基于coreAudio框架，使用缓冲队列处理音频数据，处理低延迟音频。

1. **AudioQueueRef** - 音频队列对象

2. **AudioQueueBufferRef** - 音频缓冲区

3. **AudioStreamBasicDescription** - 音频格式描述

### 1、字幕采集：

使用人工录入或语音识别录入。

### 2、字母渲染：

嵌入画面，应用层的textview显示

### 3、音视频格式转化：

YUV《》RGB

### 4、大小转化和裁剪：

480P，720P，1080P，2K，4K，8K，

美颜：磨皮，手链，瘦腿，上妆

ai画面超分辨率

ai换脸

### 5、音频处理：

(1)重采样

位宽：8位，16位，浮点，双浮点

采用率：8K，16K，32K，44.1K，48K，96K

通道：1，2，3，4，5，6，7，8通道

(2)变声处理

a.变速不变调

b.男声变女声

(3)AEC(回声消除)

(4)NS(降噪)

(5)AGC(自动增益)

.视频的渲染

### 6、Windows平台：

(1)Direct3D 9,11

(2)EVR,EVR(CP)

(3)OpenGL

### 7、Android平台：

(1)OpenGLES

### 8、Apple平台：

(1)Metal

(2)OpenGLES

### 9、Linux平台：

(1)SDL

(2)OpenGL



### 10、OpenAL

音频通用平台OpenAL

多媒体主要包括两个:声音和媒体

目前常用的开发框架：

VLC，ffmpeg，vitamio框架



视频播放器中的surface和surfaceView简介

MediaContorller

视频解码：就是将压缩后的音视频还原为原来的画质

分为硬解和软解



### 11、流媒体协议：

RTP：实时传输协议，UDP之上

RTCP：实时传输控制协议，为RTP提供质量反馈

RTSP：实时流传输协议，提供了一个可扩展框架，双向的允许倒退回放

RTMP：实时消息传输协议，基于TCP

HLS：apple实现的基于HTTP的流媒体传输协议

----

----

---

# 》》》JNI技术

可以使用java调用本地代码

可以通过本地代码调用java

可以访问操作系统的特有API，使用c/c++更快

直接操作硬件或系统调用

audioTrack怎么通过JNI来链接android音频

```java
// Java layer: AudioTrack.java (API directly used by developers)
public class AudioTrack {
    // Java layer method declaration
    public void play() {
        native_play(); // Call native method
    }
// Native method declaration (no implementation body)
private native void native_play();

// Other methods: write(), stop(), release(), etc.
```

JNI桥梁作用

链接java和c/c++

java层定义Native方法





使用：首先在java层定义native方法

```java
private native final void native_play();
```

然后再JNI层实现（c++）：



```cpp
// frameworks/base/media/jni/android_media_AudioTrack.cpp

// JNI函数：对应Java的native_play()
static void android_media_AudioTrack_play(JNIEnv *env, jobject thiz) {
    // 从Java对象中获取Native对象指针
    AudioTrack *lpTrack = getAudioTrack(env, thiz);
    if (lpTrack == NULL) {
        return;
    }
    
// 调用C++ AudioTrack的play方法
lpTrack->play();
}

```

```cpp
// JNI函数：对应Java的native_write_byte()
static jint android_media_AudioTrack_write_byte(
        JNIEnv *env, jobject thiz,
        jbyteArray javaAudioData,
        jint offsetInBytes, jint sizeInBytes,
        jint javaAudioFormat) {
// 1. 获取Java数组指针
jbyte* cAudioData = env->GetByteArrayElements(javaAudioData, NULL);

// 2. 获取Native AudioTrack对象
AudioTrack *lpTrack = getAudioTrack(env, thiz);

// 3. 调用底层write函数
ssize_t written = lpTrack->write(cAudioData + offsetInBytes, 
                                 sizeInBytes, true);

// 4. 释放Java数组
env->ReleaseByteArrayElements(javaAudioData, cAudioData, 0);

return (jint)written;
}

// JNI Native方法注册表
static const JNINativeMethod gMethods[] = {
    {"native_play", "()V", (void *)android_media_AudioTrack_play},
    {"native_write_byte", "([BIII)I", 
     (void *)android_media_AudioTrack_write_byte},
    // ... 其他方法映射
};

// 注册函数
int register_android_media_AudioTrack(JNIEnv *env) {
    return AndroidRuntime::registerNativeMethods(env,
            "android/media/AudioTrack", gMethods, NELEM(gMethods));
}
```




### 1、**AudioTrack数据流经JNI的完整过程**

```java
// 开发者代码
byte[] audioData = new byte[1024];
// ... 填充音频数据
AudioTrack audioTrack = new AudioTrack(...);

// 1. Java层调用write方法
audioTrack.write(audioData, 0, audioData.length);

// 2. Java层write()内部调用native_write_byte()
//    ↓ 通过JNI跳转到C++
// 3. JNI层：android_media_AudioTrack_write_byte()
//    - 获取Java数组的C指针
//    - 调用Native AudioTrack::write()
//    ↓ 进入C++ Native层
// 4. Native AudioTrack::write()
//    - 将数据放入AudioTrack的缓冲区
//    ↓ 进入音频服务
// 5. AudioFlinger接收数据
//    - 混音
//    - 路由到对应输出设备
// 6. HAL层 -> 内核ALSA驱动 -> 硬件DMA -> 扬声器/耳机
```



### 2、JNI在安卓音频系统中的优势

延迟优化：直接访问内存，减少拷贝，使用指针直接操作，避免java数组拷贝

精确时序控制

native层可以精确控制音频时序，直接和音频硬件时间同步，处理时间戳和延迟计算

JNI层捕获native异常转化为Java异常

开发时使用JNI

创建AudioTrack时会调用JNI，



```java
// Java代码
AudioTrack track = new AudioTrack(
    AudioManager.STREAM_MUSIC,  // streamType
    44100,                      // sampleRateInHz
    AudioFormat.CHANNEL_OUT_STEREO, // channelConfig
    AudioFormat.ENCODING_PCM_16BIT, // audioFormat
    bufferSize,                 // bufferSizeInBytes
    AudioTrack.MODE_STREAM      // mode
);
```



// 背后发生的JNI调用：
// 1. Java构造函数 → native_setup()
// 2. JNI层创建Native AudioTrack对象
// 3. 连接AudioFlinger服务
// 4. 分配共享内存缓冲区



避免频繁调用JNI

不用每次都写一个sample

直接使用批量写入

```java
track.write(data, 0, data.length);  // 一次JNI调用处理所有数据
```



### 3、JNI的一些问题

// JNI错误可能表现：
// - UnsatisfiedLinkError: native方法找不到
// - JNI DETECTED ERROR IN APPLICATION: 使用无效的JNI引用
// - 音频数据损坏：JNI内存访问越界
// - 内存泄漏：忘记ReleaseByteArrayElements



### 4、调试

#查看JNI引用表

adb shell dumpsys meminfo <package_name> | grep "JNI:"

#启用JNI检查

adb shell setprop dalvik.vm.checkjni true

#使用Android Studio的JNI调试功能





### 5、**总结：为什么AudioTrack需要JNI**

1. **性能需求**：音频渲染需要低延迟、高吞吐量，C++比Java更适合。
2. **硬件访问**：需要直接与音频驱动和硬件交互。
3. **系统服务集成**：需要与AudioFlinger等系统服务通信。
4. **实时性保证**：Native代码可以提供更可靠的实时保证。

**关键理解点**：

- **AudioTrack的Java API只是"门面"**，实际工作都在Native层完成。
- **JNI是"桥梁"**，负责Java和C++之间的数据转换和方法调用。
- **每次Java调用AudioTrack方法**，都会通过JNI进入Native世界。

对于Android音频开发者，理解JNI机制有助于：

- 优化音频应用的性能
- 调试复杂的音频问题
- 理解Android音频系统的架构
- 在必要时实现自己的JNI音频处理模块

---

---

---

# 》》》音视频播放的硬解与软解：定义、区别及适用场景

音视频解码是将压缩的音视频数据（如 H.264、H.265、AAC 格式）转换为可播放的原始数据（YUV 图像、PCM 音频）的过程，**硬解**和**软解**是实现这一过程的两种核心技术方案，核心差异在于**解码任务的执行主体**不同。

### 一、核心定义

#### 1. 软解（软件解码）

- **执行主体**：由 **CPU** 完全通过软件算法完成解码计算。
- **实现原理**：依赖纯软件库（如 FFmpeg 的 libavcodec、OpenH264），无需硬件专属解码模块，通过 CPU 指令集（如 x86 的 SSE/AVX、ARM 的 NEON）加速运算，但本质仍由 CPU 承担全部解码负载。
- **典型工具 / 框架**：FFmpeg、VLC 播放器（默认软解）、Chrome 浏览器（部分场景软解）。

#### 2. 硬解（硬件解码）

- **执行主体**：由设备内置的 **专用硬件解码模块** 完成解码（如手机的 GPU/ISP、电脑的显卡、智能电视的解码芯片）。

- **实现原理**：硬件解码模块是为音视频解码量身设计的电路，可并行处理大规模的解码计算（如 H.265 的帧内预测、变换编码），解码过程不占用 CPU 核心资源。

- 典型接口 / 标准

  ：

  - 移动端：Android 的 `MediaCodec`、iOS 的 `VideoToolbox`；
  - PC 端：NVIDIA 的 NVDEC、AMD 的 UVD、Intel 的 QSV；
  - 跨平台：OpenMAX IL（如 OpenSL ES 配套解码接口）。

### 二、核心区别对比

| 对比维度         | 软解                                        | 硬解                                                 |
| ---------------- | ------------------------------------------- | ---------------------------------------------------- |
| **核心载体**     | CPU + 软件解码库                            | 专用硬件解码模块（GPU / 解码芯片）                   |
| **CPU 占用**     | 高（尤其 4K/8K 视频）                       | 低（CPU 仅负责控制指令，不参与计算）                 |
| **功耗表现**     | 高（CPU 满负载运行发热明显）                | 低（硬件模块能效比更高）                             |
| **解码性能**     | 受 CPU 性能限制，4K/8K 高码率视频易卡顿     | 性能强，轻松支持 4K/8K 60fps 高码率视频              |
| **格式兼容性**   | 强（软件库可快速支持新格式，如 AV1）        | 弱（依赖硬件支持，老旧设备不支持 H.265/AV1）         |
| **延迟表现**     | 较高（CPU 多任务调度存在开销）              | 低（硬件直解，适合实时播放 / 直播场景）              |
| **跨平台一致性** | 强（同一软件库在不同设备表现一致）          | 弱（不同厂商硬件解码效果存在差异，易出现兼容性问题） |
| **定制化能力**   | 高（可修改解码算法，支持自定义滤镜 / 处理） | 低（硬件逻辑固定，无法自定义解码流程）               |

### 三、适用场景

#### 1. 软解的适用场景

软解的核心优势是**兼容性强、跨平台一致、可定制**，适合以下场景：

- **老旧设备播放**：老旧手机 / 电脑无专用硬件解码模块（如不支持 H.265），只能依赖 CPU 软解，确保视频能正常播放。
- **播放小众 / 新兴编码格式**：当遇到硬件不支持的新格式（如 AV1、VP9 早期版本）或小众格式（如 RMVB、MKV 内嵌罕见编码），软解是唯一选择（可通过升级软件库快速支持新格式）。
- **音视频后处理需求高的场景**：如视频剪辑软件（Premiere、剪映）、直播推流工具（OBS），需要在解码后对视频进行滤镜、缩放、水印叠加等操作。软解可直接获取 CPU 可操作的原始数据，方便后续处理；而硬解数据需从硬件缓冲区拷贝到内存，增加额外开销。
- **跨平台一致性要求高的场景**：如在线教育平台、企业视频会议软件，需要在 Windows/macOS/Linux/Android/iOS 等多平台保证播放效果一致，软解可避免不同硬件解码的兼容性问题。
- **低码率标清视频播放**：如播放 720P 及以下码率的视频，CPU 负载较低，软解完全能胜任，且无需依赖硬件，实现成本更低。

#### 2. 硬解的适用场景

硬解的核心优势是**低 CPU 占用、低功耗、高性能**，适合以下场景：

- **高码率高清视频播放**：如 4K/8K 60fps 蓝光原盘、H.265 高码率电影，这类视频解码计算量极大，CPU 软解极易卡顿，硬解可充分发挥硬件模块的并行处理能力，实现流畅播放。
- **移动端 / 嵌入式设备**：手机、平板、智能电视、机顶盒等设备电池容量有限，且 CPU 性能相对 PC 较弱。硬解可大幅降低功耗，延长续航时间，同时减少设备发热。
- **实时性要求高的场景**：如直播观看、视频通话、安防监控实时流，硬解延迟更低，能有效减少 “画面卡顿”“音画不同步” 问题。
- **多任务并行场景**：如电脑同时运行视频播放 + 游戏 + 办公软件，硬解不占用 CPU 资源，可保证其他程序正常运行；若用软解，CPU 会被解码占满，导致游戏掉帧、办公软件卡顿。
- **大规模视频分发场景**：如视频网站服务器转码、智能摄像头录像存储，硬件解码阵列可同时处理大量视频流，提升整体处理效率，降低服务器成本。

### 四、总结：硬解 vs 软解怎么选？

| 选择维度 | 优先选硬解                     | 优先选软解                       |
| -------- | ------------------------------ | -------------------------------- |
| 视频规格 | 4K/8K 高码率、高帧率           | 720P/1080P 低码率、小众格式      |
| 设备类型 | 移动端、嵌入式设备、低功耗设备 | 高性能 PC、老旧设备              |
| 业务需求 | 流畅播放、低延迟、低功耗       | 格式兼容、定制化处理、跨平台一致 |

实际应用中，主流播放器（如 MX Player、PotPlayer）会默认采用 **“硬解优先，软解兜底”** 的策略：优先尝试硬解以保证性能，若硬件不支持当前格式或出现兼容性问题，自动切换到软解。



